
# AlexNet 微调 - Caltech-101分类 实验报告

## 1. 引言

本报告介绍了将预训练的AlexNet模型应用于Caltech-101数据集的图像分类任务。实验对比了三种训练策略：微调预训练模型、从零训练模型和特征提取（仅训练最后一层），并分析了不同超参数对模型性能的影响。

## 2. 数据集介绍

### 2.1 Caltech-101数据集概述

Caltech-101是一个包含101个类别的图像数据集，每个类别有约40到800张图像。图像尺寸大约为300x200像素，以彩色形式提供。

### 2.2 数据预处理

在本实验中，为了适应AlexNet模型的输入要求，我们对Caltech-101数据集进行了以下预处理步骤：
1.图像大小调整：将所有图像调整为224×224像素，这是AlexNet模型的标准输入尺寸。
2.数据增强（仅用于训练集）：
- 随机水平翻转（概率0.5）
- 随机旋转（±10度）
- 随机调整亮度、对比度和饱和度（因子范围0.8-1.2）
- 随机裁剪到224×224像素（先缩放到256×256）
3.标准化：使用ImageNet数据集的均值和标准差对图像进行标准化：
- 均值：[0.485, 0.456, 0.406]
- 标准差：[0.229, 0.224, 0.225]
这些预处理步骤有助于提高模型的泛化能力，减少过拟合风险，并使模型能够适应不同条件下的图像。

```python
transforms.Compose([
    transforms.Resize(256),
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
```

### 2.3 数据集划分

按照Caltech-101标准，我们将数据集分为训练集和测试集：
- 训练集：每类30张图像
- 测试集：每类剩余图像

## 3. 模型架构

### 3.1 预训练的AlexNet模型

本实验使用了在ImageNet上预训练的AlexNet模型，该模型包含5个卷积层和3个全连接层。

### 3.2 模型修改

为适应Caltech-101数据集，我们修改了最后一个全连接层，将输出大小从1000（ImageNet类别数）调整为101（Caltech-101类别数）。

```python
# 加载预训练的AlexNet模型
model = models.alexnet(pretrained=True)

# 获取输入特征数量
num_features = model.classifier[6].in_features

# 替换最后一个全连接层
# 原始层: (6): Linear(in_features=4096, out_features=1000, bias=True)
# 修改为: (6): Linear(in_features=4096, out_features=101, bias=True)
model.classifier[6] = nn.Linear(num_features, 101)

# 对于特征提取模式，冻结所有卷积层和部分全连接层的参数
if feature_extract:
    for param in list(model.parameters())[:-2]:  # 除了最后一层外都冻结
        param.requires_grad = False
```

这一修改将原始的1000类输出层（对应ImageNet数据集的1000个类别）替换为101类输出层（对应Caltech-101数据集的101个类别）。在特征提取模式下，我们仅训练最后一层的参数，而在微调模式下，我们则使用较小的学习率对所有层进行训练。
对于微调策略，我们设计了差异化学习率的方法：
- 为预训练层设置较小的学习率（finetune_lr，通常为0.0001）
- 为新添加的分类层设置较大的学习率（lr，通常为0.001）
这种方法可以在保留预训练模型已习得特征的同时，让新添加的分类层能够快速适应新的分类任务。

## 4. 实验设置

### 4.1 训练策略

实验比较了三种训练策略：

1. **微调预训练模型**：使用预训练权重初始化，对所有层进行微调，但使用较小的学习率
   ```bash
    uv run main.py --mode finetune --epochs 30 --batch_size 32 --lr 0.001 --finetune_lr 0.0001 --optimizer adamw --weight_decay 0.0001 --scheduler cosine_warmup --warmup_steps 500 --save_frequency 10 --mixed_precision --early_stopping 5 --grad_clip 1.0
   ```

2. **从零训练模型**：使用随机初始化权重，从头开始训练模型
   ```bash
    uv run main.py --mode scratch --epochs 150 --batch_size 64 --lr 0.02 --optimizer adamw --weight_decay 0.001 --scheduler cosine --min_lr 1e-6 --save_frequency 20 --mixed_precision --early_stopping 20 --grad_clip 0.5
   ```

3. **特征提取**：固定预训练模型的特征提取层，只训练最后的分类层
   ```bash
    uv run main.py --mode finetune --feature_extract --epochs 30 --batch_size 32 --lr 0.001 --finetune_lr 0.0001 --optimizer adamw --weight_decay 0.0001 --scheduler cosine_warmup --warmup_steps 500 --save_frequency 10 --mixed_precision --early_stopping 5 --grad_clip 1.0
   ```

### 4.2 超参数设置

各训练策略的超参数设置如下表所示：

| 参数 | 微调模式 | 从零训练模式 | 特征提取模式 |
|------|---------|------------|--------------|
| 学习率 | 0.001 (新层)<br>0.0001 (预训练层) | 0.02 | 0.001 |
| 轮数 | 30 | 150 | 30 |
| 优化器 | AdamW | AdamW | AdamW |
| 权重衰减 | 0.0001 | 0.001 | 0.0001 |
| 学习率调度 | 余弦预热<br>(warmup_steps=500) | 余弦退火<br>(min_lr=1e-6) | 余弦预热<br>(warmup_steps=500) |
| 批次大小 | 32 | 64 | 32 |
| 早停轮数 | 5 | 20 | 5 |
| 梯度裁剪 | 1.0 | 0.5 | 1.0 |
| 保存频率 | 10 | 20 | 10 |
| 混合精度 | 是 | 是 | 是 |

这些超参数设置反映了不同训练策略的特点：

1. **微调模式**：使用较小的学习率微调预训练层以保留已学习的特征，同时对新的分类层使用较大学习率，轮数适中。

2. **从零训练模式**：由于需要从头学习所有特征，因此使用更大的学习率(0.02)、更多的训练轮数(150)、更大的批次大小(64)和更高的权重衰减(0.001)来防止过拟合，同时设置更长的早停耐心值(20)以适应训练过程中可能出现的波动。

3. **特征提取模式**：由于只训练最后的分类层，超参数设置与微调模式相似，但所有层使用统一学习率。

所有模式都采用了混合精度训练以加速计算和减少内存占用，并使用梯度裁剪来防止梯度爆炸问题。


### 4.3 评估指标

使用以下指标评估模型性能：
- 准确率（Accuracy）
- 混淆矩阵
- 分类报告（精确率、召回率、F1分数）

## 5. 实验结果

### 5.1 微调模型

#### 5.1.1 训练过程可视化

收敛速度最快，验证集准确率最高。

![微调模型训练过程——训练损失和训练准确率](./images/finetune_train_loss.png)

![微调模型训练过程——验证损失和验证准确率](./images/finetune_val_loss.png)

#### 5.1.2 性能测试

Evaluation Loss: 0.7957 Acc: 0.8453

![微调模型性能测试](./images/finetune_test.png)

### 5.2 从零训练模型

#### 5.2.1 训练过程可视化

收敛速度最慢，验证集准确率最低。

![从零训练模型训练过程——训练损失和训练准确率](./images/scratch_train_loss.png)

![从零训练模型训练过程——验证损失和验证准确率](./images/scratch_val_loss.png)

#### 5.2.2 性能测试

Evaluation Loss: 2.2082 Acc: 0.6616

![从零训练模型性能测试](./images/scratch_test.png)

### 5.3 特征提取模型

#### 5.3.1 训练过程可视化

收敛速度略慢于微调模型，验证集准确率略低于微调模型。

![特征提取模型训练过程——训练损失和训练准确率](./images/feature_extract_train_loss.png)

![特征提取模型训练过程——验证损失和验证准确率](./images/feature_extract_val_loss.png)


#### 5.3.2 性能测试

Evaluation Loss: 0.5483 Acc: 0.8453

![特征提取模型性能测试](./images/feature_extract_test.png)

### 5.4 不同超参数组合对比

#### 5.4.1 不同学习率对比

使用以下命令进行不同学习率的实验：
```bash
uv run main.py --mode finetune --epochs 20 --lr 0.001 --finetune_lr 0.001
uv run main.py --mode finetune --epochs 20 --lr 0.001 --finetune_lr 0.0001
uv run main.py --mode finetune --epochs 20 --lr 0.001 --finetune_lr 0.00001
```

结果：
- 学习率=0.001, finetune_lr=0.001: Evaluation Loss: 4.1692 Acc: 0.0935，训练损失和验证损失都很大，训练准确率和验证准确率都很低。
  - ![学习率=0.001, finetune_lr=0.001](./images/finetune_lr_0.001.png)
- 学习率=0.001, finetune_lr=0.0001: Evaluation Loss: 0.7164 Acc: 0.8480，训练损失和验证损失都最小，训练准确率和验证准确率都最高。
  - ![学习率=0.001, finetune_lr=0.0001](./images/finetune_lr_0.0001.png)
- 学习率=0.001, finetune_lr=0.00001: Evaluation Loss: 0.9964 Acc: 0.8294，训练损失和验证损失都较小，训练准确率和验证准确率都较高。
  - ![学习率=0.001, finetune_lr=0.00001](./images/finetune_lr_0.00001.png)



#### 5.4.2 不同优化器对比

使用以下命令进行不同优化器的实验：
```bash
uv run main.py --mode finetune --optimizer sgd --epochs 20
uv run main.py --mode finetune --optimizer adam --epochs 20
uv run main.py --mode finetune --optimizer adamw --epochs 20
```

结果：
- 优化器=sgd: Evaluation Loss: 0.7001 Acc: 0.8486，准确率稳定上升，波动最小。
  - ![优化器=sgd](./images/optimizer_sgd.png)
- 优化器=adam: Evaluation Loss: 0.7164 Acc: 0.8480，准确率波动较大，但验证集准确率也比较高。
  - ![优化器=adam](./images/finetune_lr_0.0001.png)
- 优化器=adamw: Evaluation Loss: 0.8101 Acc: 0.8398，准确率波动最大。
  - ![优化器=adamw](./images/optimizer_adamw.png)



## 6. 讨论与分析

### 6.1 预训练的影响

通过对比微调模型和从零训练模型的实验结果，可以明显看出预训练带来的显著提升：

1. **性能差异**：微调模型在测试集上达到了84.53%的准确率，而从零训练模型仅达到66.16%，相差约18个百分点。这表明预训练权重确实为模型提供了有价值的先验知识。

2. **收敛速度**：微调模型仅需30个epoch就能达到较高的准确率，而从零训练模型即使经过150个epoch训练，性能仍显著低于微调模型。从训练曲线可以看出，微调模型在训练初期就迅速达到了较高的准确率。

3. **泛化能力**：微调模型的验证损失明显低于从零训练模型（0.7957 vs 2.2082），说明预训练模型具有更强的泛化能力，不易过拟合。

4. **优势原因**：ImageNet数据集包含大量多样化的图像，预训练模型已经学习到了丰富的低级特征（如边缘、纹理）和中级特征（如形状、部件），这些特征对Caltech-101数据集的分类同样有效。虽然两个数据集的类别不完全相同，但特征表示具有一定的迁移性。

预训练模型之所以有效，主要是因为深度神经网络的分层特性——较低层学习通用特征，而较高层学习特定于任务的特征。通过微调，我们保留了通用特征，同时调整了特定特征以适应新任务。

### 6.2 特征提取 vs 完全微调

特征提取模型（仅训练最后一层）和完全微调模型（调整所有层参数）在本实验中表现出了一些有趣的差异：

1. **性能比较**：出人意料的是，两种方法在测试集上达到了相同的准确率（84.53%），这表明对于Caltech-101这样的数据集，预训练模型的特征提取部分已经足够强大。

2. **损失函数差异**：特征提取模型的评估损失（0.5483）明显低于完全微调模型（0.7957），这表明特征提取模型的预测置信度更高。

3. **训练效率**：特征提取模型只需更新最后一层的参数，训练速度更快、内存占用更少，同时减少了过拟合的风险。

4. **适用场景**：实验结果说明，当目标数据集与预训练数据集相似度较高，且目标数据集规模较小时，特征提取可能是更优的选择，不仅计算效率更高，还能避免过拟合。

5. **理论解释**：AlexNet在ImageNet上学习的特征对Caltech-101具有很强的迁移性，表明图像分类任务中的低层特征具有普遍适用性。对于Caltech-101这样的小型数据集，完全微调可能导致模型"忘记"预训练中学到的有用特征。

然而，特征提取方法的局限性在于无法调整特征提取器来适应目标数据集的特定特征，如果两个数据集差异较大，这种方法的性能可能会受到限制。

### 6.3 超参数影响分析

通过对不同超参数组合的实验，我们观察到以下影响：

1. **学习率影响**：
   - 当预训练层使用过高的学习率（0.001）时，模型性能严重下降（准确率仅为9.35%），这是因为过大的学习率破坏了预训练模型中有价值的特征。
   - 使用适中的学习率（0.0001）时，模型性能最佳（准确率84.80%）。
   - 过小的学习率（0.00001）导致性能略有下降（准确率82.94%），可能是因为模型无法充分适应新数据集的特征。
   
   这表明在微调过程中，学习率的选择至关重要，需要在保留预训练知识和适应新任务之间找到平衡。

2. **优化器比较**：
   - SGD优化器表现最稳定，最终准确率最高（84.86%），训练曲线波动最小。
   - Adam优化器性能接近SGD（准确率84.80%），但训练过程中波动较大。
   - AdamW优化器表现略逊（准确率83.98%），波动最大。
   
   这说明对于微调任务，SGD优化器可能是更好的选择，其稳定性有助于模型保留预训练知识并逐渐适应新任务。

3. **训练策略对比**：
   - 微调模式需要较小的学习率和适当的学习率调度（如余弦预热）以保护预训练特征。
   - 从零训练模式需要较大的学习率、更多的训练轮数和较大的批次大小，以及更强的正则化来防止过拟合。
   - 特征提取模式适合使用中等学习率和简单的学习率调度，因为参数更新较少。

总的来说，超参数选择应根据训练策略和数据集特性进行调整。对于微调任务，应当特别注意学习率的设置，尽量使用较小的学习率来更新预训练层，以保留有用的特征表示，同时使用较大的学习率来训练新添加的层，以加速适应新任务。



## 7. 附录

- GitHub 代码库: [https://github.com/Sisicca/DLMidterm]
- 训练好的模型权重（提取码: 693v）: [https://pan.baidu.com/s/1b0pGmOxVI_sbaOL6WUwSdg?pwd=693v]

